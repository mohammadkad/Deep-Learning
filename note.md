<!--
1404-06-30
Mohammad Kadkhodaei Elyaderani
-->

Book Summary: Deep Learning with Python, Third Edition, MEAP Edition, Manning Early Access Program, Copyright 2025 Manning Publications

<!-- Chapter 1.0 -->

<!-- Chapter 2.0 -->
<!-- ... -->
- tensors, tensor operations, differentiation, gradient descent, and so on.
- a tensor is a container for data – usually numerical data. So, it’s a container for numbers.
  - Scalars (rank-0 tensors)
  - Vectors (rank-1 tensors)
  - Matrices (rank-2 tensors)
  - Rank-3 tensors and higher-rank tensors
- output = relu(matmul(input, W) + b)
<!-- ... -->

<!-- Chapter 15 -->
- Language Models and the Transformer
- Attention : attention was actually developed as a way to augment an RNN model. Researchers noticed that while RNNs excel at modeling dependencies in a local neighborhood, they struggled with recall as sequences got longer.
- Attention : is a mechanism that allows a model to pull information from anywhere in a sequence selectively based on the context of the token currently being processed.
- The idea with attention is to build a mechanism by which a neural network can give more weight to some part of a sequence and less weight to others contextually, depending on the current input being processed.
- Transformers are data-hungry models.


